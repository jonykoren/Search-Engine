{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Directory\n",
    "# %cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import selenium.webdriver.support.ui as ui\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RUN for Each Topic\n",
    "link_dict = dict()\n",
    "file = \"recommender system.txt\"\n",
    "output = \"recommenderRank.txt\"\n",
    "# Define Keywords\n",
    "keywords = [\"electronic\",\"recommendation\", \"recommender\", \"technology\", \"service\", \"platform\",\"user\", \"filtering\", \"function\",'online','market','company','website','user', \"e-commerce\", \"ecommerce\", 'products', \"emtia\", 'sellers', 'items']\n",
    "print(keywords)\n",
    "\n",
    "run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all():\n",
    "    links = {}\n",
    "    link_dict = dict()\n",
    "    f = open(file, \"r\").read().split('\\n')\n",
    "    for syc, link in enumerate(f):\n",
    "        print(link)\n",
    "        response = requests.get(link)\n",
    "        print(response)\n",
    "        if response:\n",
    "            print(\"Response has been received!\")\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            try:\n",
    "                call_all(syc, link)\n",
    "            except:\n",
    "                continue\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(link):\n",
    "    data = {}\n",
    "    text_list = list()\n",
    "    urlFirst = link\n",
    "    driver = webdriver.Chrome('C:/Users/hmtkv/2nd Trimester/thesis/crawler/selen/chromedriver.exe')\n",
    "    driver.get(urlFirst)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "    wait=ui.WebDriverWait(driver,10)\n",
    "    for i in driver.find_elements_by_xpath('/html/body'): \n",
    "         text_list.append(i.text)     \n",
    "    data[\"txt\"] = text_list\n",
    "    driver.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_clean(data_combined):\n",
    "    pd.set_option('max_colwidth',150)\n",
    "    data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "    data_df.columns = ['transcript']\n",
    "    data_df = data_df.sort_index()\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(data_clean):\n",
    "    cv = CountVectorizer(stop_words='english')\n",
    "    data_cv = cv.fit_transform(data_clean.transcript)\n",
    "    data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "    data_dtm.index = data_clean.index\n",
    "    return data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_data(data_dtm,data_clean, syc):\n",
    "    link1 = \"dtm\" +  str(syc) +  \".pkl\"\n",
    "    link2 = \"data_clean\" +  str(syc) +  \".pkl\"\n",
    "    data_clean.to_pickle(link2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synsets(keywords):\n",
    "    synonyms = []\n",
    "    syns = list()\n",
    "    for i in keywords:\n",
    "        syns.append(i)\n",
    "        for syn in wordnet.synsets(str(i)):\n",
    "            for lm in syn.lemmas():\n",
    "                syns.append(lm.name())\n",
    "    [synonyms.append(syn.replace(\"_\", \" \")) for syn in syns]\n",
    "    return set(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(synonyms, data_stop):\n",
    "    counter = 0\n",
    "    for syn in synonyms:\n",
    "    #     print(syn)\n",
    "        for topword in data_stop.loc[:,'txt'].index:\n",
    "    #         print(topword)\n",
    "            if syn == topword :\n",
    "                counter += data_stop.loc[topword].values\n",
    "                print(\"counter 1\", counter)\n",
    "    return counter     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_all(syc, link):\n",
    "    # Main Kumanda\n",
    "    link2 = \"data_clean\" +  str(syc) +  \".pkl\"\n",
    "    data = get_text(link)\n",
    "    # Combine\n",
    "    data_combined = {key: [combine_text(value)] for (key, value) in data.items()}\n",
    "    # Clean data\n",
    "    data_df = get_df_clean(data_combined)\n",
    "    # Clean round 1\n",
    "    data_clean1 = pd.DataFrame(data_df.transcript.apply(round1))\n",
    "    # Clean round 2\n",
    "    data_clean = pd.DataFrame(data_clean1.transcript.apply(round2))\n",
    "    #stop words\n",
    "    data_dtm = stopwords(data_clean)\n",
    "    #Put data to pickle\n",
    "    pickle_data(data_dtm,data_clean, syc)\n",
    "    #Get vectorized count \n",
    "    data_clean = pd.read_pickle(link2)  \n",
    "    link_dict = deneme(data_clean, syc)\n",
    "    #Save to file\n",
    "    append(link_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deneme(data_clean, syc):\n",
    "    from sklearn.feature_extraction import text \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    stop_words = text.ENGLISH_STOP_WORDS\n",
    "    cv = CountVectorizer(stop_words=stop_words)\n",
    "    data_cv = cv.fit_transform(data_clean.transcript)\n",
    "    data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "    data_stop.index = data_clean.index\n",
    "    data_stop = data_stop.T.sort_values(by = 'txt', ascending = False)\n",
    "    #Get Synonyms\n",
    "    synonyms = get_synsets(keywords)\n",
    "    # Finally get Counter for this link\n",
    "    counter = get_count(synonyms, data_stop)\n",
    "    print(counter)\n",
    "    print(syc)\n",
    "    link_dict[syc] = counter\n",
    "    print(link_dict)\n",
    "    return link_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append(link_dict):\n",
    "    a_file = open(output, \"a\")\n",
    "    writer = csv.writer(a_file)\n",
    "    for key, value in link_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "    a_file.close()\n",
    "    print(\"appended!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
